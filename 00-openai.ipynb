{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqTmpkl_5Wij"
      },
      "source": [
        "### L’API OpenAI\n",
        "\n",
        "#### Installation des dépendences\n",
        "\n",
        "Pour la clé d'api OpenAI, il est indispensable de la sécuriser. Pour cela plusieurs possibilités selon le contexte:\n",
        "\n",
        "* Via Google Colab: créez un secret Google Colab nommé `OPENAI_API_KEY` (menu de gauche) puis utiliser `google.colab.userdata.get` pour la récupérer\n",
        "\n",
        "* En local: créez un fichier `.env`, déclarez une paire clé-valeur `OPENAI_API_KEY=votre_clé` puis utiliser `python-dotenv` pour la charger dans les variables d'environnement\n",
        "\n",
        "Quoi qu'il arrive ne **JAMAIS** partager ou commiter vos clés d'api.\n",
        "\n",
        "En dehors de la gestion de la clé, nous installons le wrapper de l'api OpenAI."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install openai -q\n",
        "#%pip install python-dot-env -q"
      ],
      "metadata": {
        "id": "4YnmObqL6Crf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_-3CISb5Win"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Clé depuis secrets Google Colab\n",
        "Setter la clé dans les secrets dans le menu de gauche\n",
        "\"\"\"\n",
        "from google.colab import userdata\n",
        "api_key=userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "\"\"\"\n",
        "Clé OPENAI_API_KEY depuis fichier .env local.\n",
        "Uncomment les lignes suivantes\n",
        "\"\"\"\n",
        "#from dotenv import load_dotenv\n",
        "#from os import getenv\n",
        "#load_dotenv()\n",
        "#api_key=getenv('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4KcsQJJ5Wio"
      },
      "source": [
        "#### Création de la requête\n",
        "\n",
        "* temperature - entre 0 et 1. Une valeur haute, comme 0.8 entraînera une complétion plus aléatoire. Alors qu'une valeur faible, comme 0.2 donnera une complétion plus déterminste\n",
        "* messages - La liste des messages de la conversation\n",
        "* max_completion_tokens - Permet de limiter le nombre de token de la complétion, pour pouvoir par exemple contrôler le coût de la génération.\n",
        "\n",
        "> [API reference](https://platform.openai.com/docs/api-reference/chat/create)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4N6rDmdI5Wio"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Explique moi la théorie de la relativité en termes simples\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"gpt-4o-mini\"\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHH9pVVw5Wip"
      },
      "source": [
        "La réponse contient d'autres infos en plus de la réponse du LLM. Notamment les stats d'usage pour l'appel (nombre de token du prompt input et de la complétion)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TihrvNGB5Wip"
      },
      "outputs": [],
      "source": [
        "chat_completion"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_completion.usage"
      ],
      "metadata": {
        "id": "jYsOkN7P6u_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La complétion est, elle, accessible dans l'objet sous le chemin `choices[0].message.content`"
      ],
      "metadata": {
        "id": "MJ4biaCp6zCI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJ9Ac-6O5Wip"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "display(Markdown(chat_completion.choices[0].message.content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F31VlVtk5Wip"
      },
      "source": [
        "## Cas pratique :\n",
        "\n",
        "1. Créer des requêtes simples vers un modèle de langage (LLM) via l’API.\n",
        "2. Analyser les résultats générés en fonction des prompts et comprendre comment le modèle traite l'information.\n",
        "3. Expérimenter différents types de prompts pour explorer les capacités du modèle et voir comment les réponses varient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEiEMCo55Wiq"
      },
      "source": [
        "### Exercice 1 : Prompts descriptifs\n",
        "Envoyez un prompt pour obtenir une explication simple d'un concept technique.\n",
        "prompt = \"Explique-moi le fonctionnement d'un moteur à combustion interne.\"\n",
        "\n",
        "### Exercice 2: Questions ouvertes vs fermées\n",
        "Posez des questions ouvertes et des questions fermées pour voir comment le modèle réagit.\n",
        "prompt = \"Quels sont les défis actuels du changement climatique ?\"\n",
        "prompt = \"Le changement climatique affecte-t-il la montée du niveau de la mer ?\"\n",
        "\n",
        "### Exercice 3: Génération créative\n",
        "Essayez de jouer avec la température (ex. 0.3 vs 0.9) et observez les différences :\n",
        "prompt = \"En 3 phrases, rRaconte une courte histoire où un robot devient ami avec un humain.\"\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}