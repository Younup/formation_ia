{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqTmpkl_5Wij"
      },
      "source": [
        "## L’API OpenAI\n",
        "\n",
        "### Installation des dépendences\n",
        "\n",
        "#### Gestion de l'installation\n",
        "\n",
        "Selon le contexte, l'installation des dépendances diffère.\n",
        "\n",
        "* Via Google Colab, installation directement depuis le notebook via `%pip install mon-package -q`. Attention, installation à refaire dans chaque nouveau notebook\n",
        "\n",
        "* En local, le plus simple est d'utiliser un environnement virtuel python conjointement avec un fichier `requirements.txt`\n",
        "``` bash\n",
        "python -m venv .venv\n",
        "source .venv/bin/activate\n",
        "pip install -r requirements.txt\n",
        "```\n",
        "\n",
        "\n",
        "#### Gestion des clés d'API\n",
        "\n",
        "Pour la clé d'api OpenAI, il est indispensable de la sécuriser. Pour cela plusieurs possibilités selon le contexte:\n",
        "\n",
        "* Via Google Colab: créez un secret Google Colab nommé `OPENAI_API_KEY` (menu de gauche) puis utiliser `google.colab.userdata.get` pour la récupérer\n",
        "\n",
        "* En local: créez un fichier `.env`, déclarez une paire clé-valeur `OPENAI_API_KEY=votre_clé` puis utiliser `python-dotenv` pour la charger dans les variables d'environnement\n",
        "\n",
        "Quoi qu'il arrive ne **JAMAIS** partager ou commiter vos clés d'api.\n",
        "\n",
        "En dehors de la gestion de la clé, nous installons le wrapper de l'api OpenAI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YnmObqL6Crf"
      },
      "outputs": [],
      "source": [
        "#Décommenter sur Google Colab\n",
        "#%pip install openai -q\n",
        "#%pip install python-dot-env -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_-3CISb5Win"
      },
      "outputs": [],
      "source": [
        "# Décommenter pour Google Colab\n",
        "#from google.colab import userdata\n",
        "#api_key=userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "#Décommenter pour Jupyter Notebook\n",
        "from dotenv import load_dotenv\n",
        "from os import getenv\n",
        "load_dotenv()\n",
        "api_key=getenv('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4KcsQJJ5Wio"
      },
      "source": [
        "### Requête de complétion\n",
        "\n",
        "* temperature - entre 0 et 1. Une valeur haute, comme 0.8 entraînera une complétion plus aléatoire. Alors qu'une valeur faible, comme 0.2 donnera une complétion plus déterminste\n",
        "* max_completion_tokens - Permet de limiter le nombre de token de la complétion, pour pouvoir par exemple contrôler le coût de la génération.\n",
        "\n",
        "#### Endpoint principal - Responses API\n",
        "\n",
        "API principale pour manipuler un modèle OpenAI.\n",
        "\n",
        "> [Responses API reference](https://platform.openai.com/docs/api-reference/responses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=api_key)\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    instructions=\"You are a coding assistant that talks like a pirate.\",\n",
        "    input=\"How do I check if a Python object is an instance of a class?\",\n",
        ")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La réponse contient d'autres infos en plus de la réponse du LLM. Notamment les stats d'usage pour l'appel (nombre de token du prompt input et de la complétion)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Response content :\\n\", response.output_text)\n",
        "print(\"\\n\\n\")\n",
        "print(\"Response usage :\\n\", response.usage)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Endpoint legacy - Chat completions API\n",
        "\n",
        "API *legacy* pour manipuler un modèle OpenAI (supporté indéfiniment).\n",
        "\n",
        "> [Chat completions API reference](https://platform.openai.com/docs/api-reference/chat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4N6rDmdI5Wio"
      },
      "outputs": [],
      "source": [
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a coding assistant that talks like a pirate.\",\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"How do I check if a Python object is an instance of a class?\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"gpt-4o-mini\"\n",
        ")\n",
        "\n",
        "print(chat_completion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHH9pVVw5Wip"
      },
      "source": [
        "La réponse contient d'autres infos en plus de la réponse du LLM. Notamment les stats d'usage pour l'appel (nombre de token du prompt input et de la complétion)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TihrvNGB5Wip"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(\"Response content :\\n\", chat_completion.choices[0].message.content)\n",
        "print(\"\\n\\n\")\n",
        "print(\"Response usage :\\n\", chat_completion.usage)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Requête de complétion multimodal - Vision\n",
        "\n",
        "Les endpoints Responses supporte également les inputs mutli-modaux (texte + image). \n",
        "\n",
        "* Via l'url de l'image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"What is in this image?\"\n",
        "img_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d5/2023_06_08_Raccoon1.jpg/1599px-2023_06_08_Raccoon1.jpg\"\n",
        "\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    input=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"input_text\", \"text\": prompt},\n",
        "                {\"type\": \"input_image\", \"image_url\": f\"{img_url}\"},\n",
        "            ],\n",
        "        }\n",
        "    ],\n",
        ")\n",
        "\n",
        "print(\"Response content :\\n\", response.output_text)\n",
        "print(\"\\n\\n\")\n",
        "print(\"Response usage :\\n\", response.usage)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Ou via son contenu en base 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import base64\n",
        "\n",
        "prompt = \"What is in this image?\"\n",
        "with open(\"media/mcp.png\", \"rb\") as image_file:\n",
        "    b64_image = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
        "\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    input=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"input_text\", \"text\": prompt},\n",
        "                {\"type\": \"input_image\", \"image_url\": f\"data:image/png;base64,{b64_image}\"},\n",
        "            ],\n",
        "        }\n",
        "    ],\n",
        ")\n",
        "print(\"Response content :\\n\", response.output_text)\n",
        "print(\"\\n\\n\")\n",
        "print(\"Response usage :\\n\", response.usage)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Streaming\n",
        "\n",
        "L'api supporte le streaming *out-of-the-box*. La complétion est streamée, token par token, au fur et à mesure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stream = client.responses.create(\n",
        "    model=\"gpt-4o\",\n",
        "    input=\"Write a one-sentence bedtime story about a unicorn.\",\n",
        "    stream=True,\n",
        ")\n",
        "\n",
        "for event in stream:\n",
        "    print(event)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F31VlVtk5Wip"
      },
      "source": [
        "## Cas pratique :\n",
        "\n",
        "1. Créer des requêtes simples vers un modèle de langage (LLM) via l’API.\n",
        "2. Analyser les résultats générés en fonction des prompts et comprendre comment le modèle traite l'information.\n",
        "3. Expérimenter différents types de prompts pour explorer les capacités du modèle et voir comment les réponses varient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEiEMCo55Wiq"
      },
      "source": [
        "### Exercice 1 : Prompts descriptifs\n",
        "Envoyez un prompt pour obtenir une explication simple d'un concept technique.\n",
        "prompt = \"Explique-moi le fonctionnement d'un moteur à combustion interne.\"\n",
        "\n",
        "### Exercice 2: Questions ouvertes vs fermées\n",
        "Posez des questions ouvertes et des questions fermées pour voir comment le modèle réagit.\n",
        "prompt = \"Quels sont les défis actuels du changement climatique ?\"\n",
        "prompt = \"Le changement climatique affecte-t-il la montée du niveau de la mer ?\"\n",
        "\n",
        "### Exercice 3: Génération créative\n",
        "Essayez de jouer avec la température (ex. 0.3 vs 0.9) et observez les différences :\n",
        "prompt = \"En 3 phrases, raconte une courte histoire où un robot devient ami avec un humain.\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
